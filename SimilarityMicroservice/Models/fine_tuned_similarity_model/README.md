---
tags:
- sentence-transformers
- sentence-similarity
- feature-extraction
- generated_from_trainer
- dataset_size:2000
- loss:CosineSimilarityLoss
base_model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
widget:
- source_sentence: ‚Äî –±–æ–∂–µ, –µ–Ω–¥—Ä—é, ‚Äî –ø—Ä–æ—Å—Ç–æ–≥–Ω–∞–≤ –Ω—ñ–∫—ñ. –º–µ—Ç—Ç –æ–±–≤—ñ–≤ –æ—á–∏–º–∞ –≤—Å—ñ—Ö —Ç—Ä—å–æ—Ö.
  sentences:
  - ‚Äî —Å–ø–∞—Å–∏–±—ñ —Ç–æ–±—ñ , –Ω—ñ–ª–µ , –∑–∞ —Ç–µ , —â–æ —Ç–∏ —Ç–∞–∫–∏–π –ó–≤–∏—á–∞–π–Ω–∏–π . –¥–≤–∞ —Ç–≤–æ—ó —Å–ª–æ–≤–∞ –¢—ñ–ª—å–∫–∏-–Ω–æ
    –ø—Ä–∏–Ω–µ—Å–ª–∏ –º–µ–Ω—ñ –¥–µ—Å—è—Ç—å –±–∞–∫—Å—ñ–≤ . ‚Äî –°—É–≤–æ—Ä–æ ? —ñ –∑ –∫–∏–º –∂–µ —Ç–∏ —Å–ø–µ—Ä–µ—á–∞–≤—Å—è ?
  - –Ω–µ –∑–Ω–∞—é, —Ö—Ç–æ —Ä–æ–∑–ø–æ–≤—ñ–≤ –Ω–∞–∑–≤–∞–Ω–∏–º–±–∞—Ç—å–∫–∞–º –µ–Ω–¥—Ä—é –ø—Ä–æ –∞–∞—Ä–æ–Ω–∞ ‚Äî —Å–∞–º –µ–Ω–¥—Ä—é —á–∏ —Ç–æ–π –∫–æ–ø,
    —Ñ—ñ–ª —Ö—ñ“ë“ë—ñ–Ω—Å, ‚Äî –∞–ª–µ –Ω–∞–∑–≤–∞–Ω–∞ –º–∞—Ç–∏ –µ–Ω–¥—Ä—é –Ω–∞–¥—ñ—Å–ª–∞–ª–∞ –∞–∞—Ä–æ–Ω–æ–≤—ñ –ª–∏—Å—Ç–∞.
  - –Ω—ñ–ª –≤–∏—Ä—É—à–∏–≤ –¥–∞–ª—ñ –Ω–∞ —Ä–æ–∑–≤—ñ–¥–∫—É, –∞ –∫–µ–≤—ñ–Ω –∑ –∞–∞—Ä–æ–Ω–æ–º –∑–∞–ª–∏—à–∏–ª–∏—Å—è –≤ –∫—ñ–º–Ω–∞—Ç—ñ.
- source_sentence: '‚Äî –ì–∞—Ä–∞–∑–¥ , ‚Äî —Å–∫–∞–∑–∞–≤ –Ω—ñ–ª . —Ä–æ–∑—É–º—ñ—é—á–∏ , —â–æ —Ç–µ–º–∞ –±–æ–ª—é—á–∞ , –≤—ñ–Ω –≤—Å–µ
    –∂ —Ç–∞–∫–∏ –Ω–µ –≤—Ç—Ä–∏–º–∞–≤—Å—è : ‚Äî —è–∫ –¥—É–º–∞—î—à , –µ–Ω–¥—Ä—é –∑–∞–∫–∏–Ω–µ—Ç—å—Å—è —Ç–∞–±–ª–µ—Ç–∫–∞–º–∏ –ø–µ—Ä–µ–¥ –¥—Ä—É–≥–∏–º —Ç–∞–π–º–æ–º
    ? ‚Äî –Ω—ñ , ‚Äî –ø–æ—Ö–º—É—Ä–æ –æ–∑–≤–∞–≤—Å—è –∫–µ–≤—ñ–Ω .'
  sentences:
  - –ü–æ—Ä—É –≤—ñ–¥ —á–∞—Å—É –≤–æ–Ω–∏ –∑ —Ü—ñ–∫–∞–≤—ñ—Å—Ç—é –ø–æ–≥–ª—è–¥–∞–ª–∏ –Ω–∞ –∞–∞—Ä–æ–Ω–∞ —Ç–∞ –Ω—ñ–∫—ñ , –∞–ª–µ –Ω—ñ –ø—Ä–æ —â–æ –Ω–µ
    –ø–∏—Ç–∞–ª–∏ .
  - –ø—Ä–æ–±–ª–µ–º–Ω–æ , –ó–≤–∏—á–∞–π–Ω–æ , –∑–∞—Ç–µ –¥–æ—Å—ñ –¥–æ–ø–æ–º–∞–≥–∞–ª–æ –∑–∞–ª–∏—à–∞—Ç–∏—Å—è –∂–∏–≤–∏–º .
  - '‚Äî –¥–æ–±—Ä–µ, ‚Äî —Å–∫–∞–∑–∞–≤ –Ω—ñ–ª. —Ä–æ–∑—É–º—ñ—é—á–∏, —â–æ —Ç–µ–º–∞ –±–æ–ª—é—á–∞, –≤—ñ–Ω –≤—Å–µ –∂ —Ç–∞–∫–∏ –Ω–µ –≤—Ç—Ä–∏–º–∞–≤—Å—è:
    ‚Äî —è–∫ –¥—É–º–∞—î—à, –µ–Ω–¥—Ä—é –∑–∞–∫–∏–Ω–µ—Ç—å—Å—è —Ç–∞–±–ª–µ—Ç–∫–∞–º–∏ –ø–µ—Ä–µ–¥ –¥—Ä—É–≥–∏–º —Ç–∞–π–º–æ–º? ‚Äî –Ω—ñ, ‚Äî –ø–æ—Ö–º—É—Ä–æ
    –æ–∑–≤–∞–≤—Å—è –∫–µ–≤—ñ–Ω.'
- source_sentence: '–Ω—ñ–ª —Ü–µ –ø–æ–º—ñ—Ç–∏–≤, –∞–ª–µ –Ω–µ –¥—É–º–∞–≤, —â–æ –ø–æ–º—ñ—Ç—è—Ç—å —ñ–Ω—à—ñ. —Ç–µ, —â–æ –ª–∏—Å–∏ –∑–≤–µ—Ä–Ω—É–ª–∏
    –Ω–∞ —Ü–µ —É–≤–∞–≥—É —ñ –Ω–∞–º–∞–≥–∞—é—Ç—å—Å—è –π–æ–º—É –¥–æ–ø–æ–º–æ–≥—Ç–∏, –π–æ–≥–æ –≤—Ä–∞–∑–∏–ª–æ. –≤—ñ–Ω –∑–∞–º''—è–≤—Å—è –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é,
    –Ω–µ –∑–Ω–∞—Ö–æ–¥—è—á–∏ —Å–ª—ñ–≤, —ñ –∑—Ä–µ—à—Ç–æ—é –≤–∏–¥–∞–≤–∏–≤: ‚Äî –≤–∏ —Å–ø—Ä–∞–≤–¥—ñ –Ω–µ –ø—Ä–æ—Ç–∏?'
  sentences:
  - –Ω—ñ–ª —Å–ø–∞–Ω—Ç–µ–ª–∏—á–µ–Ω–æ –ø–æ–¥–∏–≤–∏–≤—Å—è –Ω–∞ –æ–±–æ—Ö.
  - ‚Äî –º–æ–∂–µ—Ç–µ –ø–µ—Ä–µ–Ω–æ—á—É–≤–∞—Ç–∏ –≤ –Ω–∞—Å, —è–∫—â–æ —Ö–æ—á–µ—Ç–µ. —è–∫—â–æ –≤–∞–º –∑–¥–∞—î—Ç—å—Å—è, —â–æ‚Ä¶ ‚Äî –≤–æ–Ω–∞ –Ω–µ –∑–∞–∫—ñ–Ω—á–∏–ª–∞,
    –∞–ª–µ –ø–æ–≥–ª—è–¥, —è–∫–∏–º –≤–æ–Ω–∞ –æ–±–≤–µ–ª–∞ –∫—ñ–º–Ω–∞—Ç—É, –∫–∞–∑–∞–≤ –±—ñ–ª—å—à–µ –∑–∞ –≤—Å—è–∫—ñ —Å–ª–æ–≤–∞.
  - '–≤–∞–π–º–∞–∫–∞ –∑ –Ω–∏–º–∏ –Ω–µ –±—É–ª–æ ‚Äî –ø–µ–≤–Ω–µ, —Å–≤–æ—é —á–∞—Å—Ç–∏–Ω—É —ñ–Ω—Ç–µ—Ä–≤''—é –≤—ñ–Ω —É–∂–µ –≤—ñ–¥–ø—Ä–∞—Ü—é–≤–∞–≤. –ø–æ–º—ñ—Ç–∏–≤—à–∏
    —Ä—É—Ö —É –∫–æ—Ä–∏–¥–æ—Ä—ñ, —Ä–µ–Ω–µ –∫–∏–Ω—É–ª–∞ –ø–æ–≥–ª—è–¥ —É –¥–≤–µ—Ä–Ω–∏–π –æ—Ç–≤—ñ—Ä —ñ –ø–æ—Å–º—ñ—Ö–Ω—É–ª–∞—Å—è –Ω—ñ–ª—É: –º–æ–≤–ª—è–≤,
    –∑—Ä–æ–∑—É–º—ñ–ª–∞. –ø—ñ—Å–ª—è —Ü—å–æ–≥–æ –≤—ñ–Ω –≤–≤–∞–∂–∞–≤ –∑–∞ –∫—Ä–∞—â–µ —è–∫–Ω–∞–π—à–≤–∏–¥—à–µ –∑–∞–±—Ä–∞—Ç–∏—Å—è ‚Äî –Ω–µ –¥–∞–π –±–æ–∂–µ
    —Å—Ö–æ–ø–ª—è—Ç—å –∂—É—Ä–Ω–∞–ª—ñ—Å—Ç–∏. —Ö–æ–≤–∞—Ç–∏—Å—è –±—É–ª–æ –æ—Å–æ–±–ª–∏–≤–æ –Ω—ñ–¥–µ, –∞–ª–µ —Ç—É—Ç –≤—ñ–Ω –ø–æ–º—ñ—Ç–∏–≤ –ø—Ä–æ—á–∏–Ω–µ–Ω—ñ
    –¥–≤–µ—Ä—ñ –º–µ–¥–∏—á–Ω–æ–≥–æ –∫–∞–±—ñ–Ω–µ—Ç—É.'
- source_sentence: ‚Äî –∑–∞–æ–¥–Ω–æ —ñ –ø–æ–¥–∏–≤–∏–º–æ—Å—è. –≤–∞–π–º–∞–∫ –∑–∞–¥–æ–≤–æ–ª–µ–Ω–æ –∫–∏–≤–Ω—É–≤ –≥–æ–ª–æ–≤–æ—é.
  sentences:
  - –≤—ñ–Ω –∑—ñ–∑–Ω–∞–≤—Å—è –º–∞—Ç–µ—Ä—ñ, —â–æ –ø—ñ–¥—Å–ª—É—Ö–∞–≤?
  - –∑–¥–∞–≤–∞–ª–æ—Å—è, –Ω—ñ–±–∏ –∑–∞ –≤—ñ–∫–Ω–æ–º –Ω–∞—Å—Ç–∞–≤ –≥–µ–ª–ª–æ–≤—ñ–Ω ‚Äî —Ç—ñ–ª—å–∫–∏ –Ω–∞ –¥–≤–∞ –º—ñ—Å—è—Ü—ñ —Ä–∞–Ω—ñ—à–µ, –Ω—ñ–∂ —Ç—Ä–µ–±–∞.
  - –º–µ—Ç—Ç –ø–µ—Ä–µ–ø–ª—ñ–≤ —Å–≤–æ—ó –ø–∞–ª—å—Ü—ñ –∑ —ó—ó –ø–∞–ª—å—Ü—è–º–∏ –π –º—ñ—Ü–Ω–æ —ó—Ö —Å—Ç–∏—Å–Ω—É–≤.
- source_sentence: '—Ä—ñ—à–µ–Ω–Ω—è –∑–¥–∞–≤–∞–ª–æ—Å—è –ø—Ä–æ—Å—Ç–∏–º —ñ –æ—á–µ–≤–∏–¥–Ω–∏–º: —Å—å–æ–≥–æ–¥–Ω—ñ –≤—ñ–Ω –Ω–µ –ø–æ–≤–∏–Ω–µ–Ω
    –ø—Ä–æ–ø—É—Å–∫–∞—Ç–∏ –ø—Ä–∏–π–æ–º —Ç–∞–±–ª–µ—Ç–æ–∫, –ø–æ–¥–æ–±–∞—î—Ç—å—Å—è –π–æ–º—É —Ü–µ —á–∏ –Ω—ñ, –ø—Ä–æ—Ç–µ ¬´–ø—Ä–æ—Å—Ç–æ¬ª –∑ –µ–Ω–¥—Ä—é
    –º—ñ–Ω—å—è—Ä–¥–æ–º –Ω–µ –±—É–ª–æ –Ω—ñ–∫–æ–ª–∏. ‚Äî –º–∞–±—É—Ç—å, —Ç–∞–∫, ‚Äî –æ—Ö–æ—á–µ –ø–æ–≥–æ–¥–∏–≤—Å—è –µ–Ω–¥—Ä—é.'
  sentences:
  - '—Ä—ñ—à–µ–Ω–Ω—è –∑–¥–∞–≤–∞–ª–æ—Å—è –ø—Ä–æ—Å—Ç–∏–º —ñ –Ø–≤–Ω–∏–π : —Å—å–æ–≥–æ–¥–Ω—ñ –≤—ñ–Ω –Ω–µ –ø–æ–≤–∏–Ω–µ–Ω –ø—Ä–æ–ø—É—Å–∫–∞—Ç–∏ –ø—Ä–∏–π–æ–º
    —Ç–∞–±–ª–µ—Ç–æ–∫ , –ø–æ–¥–æ–±–∞—î—Ç—å—Å—è –π–æ–º—É —Ü–µ —á–∏ –Ω—ñ , –ø—Ä–æ—Ç–µ ¬´ –ø—Ä–æ—Å—Ç–æ ¬ª –∑ –µ–Ω–¥—Ä—é –º—ñ–Ω—å—è—Ä–¥–æ–º –Ω–µ –±—É–ª–æ
    –Ω—ñ–∫–æ–ª–∏ . ‚Äî –º–∞–±—É—Ç—å , —Ç–∞–∫ , ‚Äî –æ—Ö–æ—á–µ –ø–æ–≥–æ–¥–∏–≤—Å—è –î–∏–≤–∞–∫ .'
  - –Ω—ñ–ª—É –ø–æ–¥–æ–±–∞–ª–æ—Å—è –≤–∏–ø–µ—Ä–µ–¥–∂–∞—Ç–∏ –π –æ–±–≤–æ–¥–∏—Ç–∏ –¥–æ–≤–∫–æ–ª–∞ –ø–∞–ª—å—Ü—è –∑–∞—Ö–∏—Å—Ç –ø—Ä–æ—Ç–∏–≤–Ω–∏–∫–∞, –ø–æ–¥–æ–±–∞–ª–æ—Å—è
    –≤—ñ–¥—á—É–≤–∞—Ç–∏ –≤–∏–∫–∏–¥ –∞–¥—Ä–µ–Ω–∞–ª—ñ–Ω—É –≤ –∫—Ä–æ–≤ –ø—ñ—Å–ª—è —Ç–æ—á–Ω–æ–≥–æ —É–¥–∞—Ä—É –ø–æ –≤–æ—Ä–æ—Ç–∞—Ö. –≤—ñ–Ω –ª—é–±–∏–≤ –¥–∏–Ω–∞–º—ñ–∫—É,
    —Ç–∏—Å–∫, —Ç—Ä—ñ—É–º—Ñ –ø–µ—Ä–µ–º–æ–≥–∏. –≤—Å–µ —ñ–Ω—à–µ –π–æ–≥–æ –∂–∏—Ç—Ç—è —è–≤–ª—è–ª–æ —Å–æ–±–æ—é —Å—É—Ü—ñ–ª—å–Ω–∏–π –±–∞—Ä–¥–∞–∫, —Ç–æ–º—É
    –¥—Ä–∞–π–≤ —Ç–∞ —á—ñ—Ç–∫—ñ –ø—Ä–∞–≤–∏–ª–∞ –≥—Ä–∏ —Å–ª—É–∂–∏–ª–∏ –π–æ–º—É –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—é –æ–ø–æ—Ä–æ—é.
  - ‚Äî —â–æ —è –ø—Ä–æ–ø—É—Å—Ç–∏–≤ ?
pipeline_tag: sentence-similarity
library_name: sentence-transformers
---

# SentenceTransformer based on sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

This is a [sentence-transformers](https://www.SBERT.net) model finetuned from [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2). It maps sentences & paragraphs to a 384-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.

## Model Details

### Model Description
- **Model Type:** Sentence Transformer
- **Base model:** [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) <!-- at revision 86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d -->
- **Maximum Sequence Length:** 128 tokens
- **Output Dimensionality:** 384 dimensions
- **Similarity Function:** Cosine Similarity
<!-- - **Training Dataset:** Unknown -->
<!-- - **Language:** Unknown -->
<!-- - **License:** Unknown -->

### Model Sources

- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)

### Full Model Architecture

```
SentenceTransformer(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
)
```

## Usage

### Direct Usage (Sentence Transformers)

First install the Sentence Transformers library:

```bash
pip install -U sentence-transformers
```

Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer

# Download from the ü§ó Hub
model = SentenceTransformer("sentence_transformers_model_id")
# Run inference
sentences = [
    '—Ä—ñ—à–µ–Ω–Ω—è –∑–¥–∞–≤–∞–ª–æ—Å—è –ø—Ä–æ—Å—Ç–∏–º —ñ –æ—á–µ–≤–∏–¥–Ω–∏–º: —Å—å–æ–≥–æ–¥–Ω—ñ –≤—ñ–Ω –Ω–µ –ø–æ–≤–∏–Ω–µ–Ω –ø—Ä–æ–ø—É—Å–∫–∞—Ç–∏ –ø—Ä–∏–π–æ–º —Ç–∞–±–ª–µ—Ç–æ–∫, –ø–æ–¥–æ–±–∞—î—Ç—å—Å—è –π–æ–º—É —Ü–µ —á–∏ –Ω—ñ, –ø—Ä–æ—Ç–µ ¬´–ø—Ä–æ—Å—Ç–æ¬ª –∑ –µ–Ω–¥—Ä—é –º—ñ–Ω—å—è—Ä–¥–æ–º –Ω–µ –±—É–ª–æ –Ω—ñ–∫–æ–ª–∏. ‚Äî –º–∞–±—É—Ç—å, —Ç–∞–∫, ‚Äî –æ—Ö–æ—á–µ –ø–æ–≥–æ–¥–∏–≤—Å—è –µ–Ω–¥—Ä—é.',
    '—Ä—ñ—à–µ–Ω–Ω—è –∑–¥–∞–≤–∞–ª–æ—Å—è –ø—Ä–æ—Å—Ç–∏–º —ñ –Ø–≤–Ω–∏–π : —Å—å–æ–≥–æ–¥–Ω—ñ –≤—ñ–Ω –Ω–µ –ø–æ–≤–∏–Ω–µ–Ω –ø—Ä–æ–ø—É—Å–∫–∞—Ç–∏ –ø—Ä–∏–π–æ–º —Ç–∞–±–ª–µ—Ç–æ–∫ , –ø–æ–¥–æ–±–∞—î—Ç—å—Å—è –π–æ–º—É —Ü–µ —á–∏ –Ω—ñ , –ø—Ä–æ—Ç–µ ¬´ –ø—Ä–æ—Å—Ç–æ ¬ª –∑ –µ–Ω–¥—Ä—é –º—ñ–Ω—å—è—Ä–¥–æ–º –Ω–µ –±—É–ª–æ –Ω—ñ–∫–æ–ª–∏ . ‚Äî –º–∞–±—É—Ç—å , —Ç–∞–∫ , ‚Äî –æ—Ö–æ—á–µ –ø–æ–≥–æ–¥–∏–≤—Å—è –î–∏–≤–∞–∫ .',
    '‚Äî —â–æ —è –ø—Ä–æ–ø—É—Å—Ç–∏–≤ ?',
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 384]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [3, 3]
```

<!--
### Direct Usage (Transformers)

<details><summary>Click to see the direct usage in Transformers</summary>

</details>
-->

<!--
### Downstream Usage (Sentence Transformers)

You can finetune this model on your own dataset.

<details><summary>Click to expand</summary>

</details>
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

## Training Details

### Training Dataset

#### Unnamed Dataset

* Size: 2,000 training samples
* Columns: <code>sentence_0</code>, <code>sentence_1</code>, and <code>label</code>
* Approximate statistics based on the first 1000 samples:
  |         | sentence_0                                                                         | sentence_1                                                                         | label                                                          |
  |:--------|:-----------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------|:---------------------------------------------------------------|
  | type    | string                                                                             | string                                                                             | float                                                          |
  | details | <ul><li>min: 7 tokens</li><li>mean: 46.04 tokens</li><li>max: 128 tokens</li></ul> | <ul><li>min: 7 tokens</li><li>mean: 46.02 tokens</li><li>max: 128 tokens</li></ul> | <ul><li>min: 0.0</li><li>mean: 0.39</li><li>max: 1.0</li></ul> |
* Samples:
  | sentence_0                                                                                                                                                                                                            | sentence_1                                                                                                                                                          | label                           |
  |:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------|
  | <code>‚Äî –∞ —Ç–µ–ø–µ—Ä –≥–æ–¥—ñ –¥–∑—è–≤–∫–∞—Ç–∏, –≤—Å—ñ —Å–ª—É—Ö–∞—î–º–æ –º–µ–Ω–µ. –ø–µ—Ä–µ–¥ –Ω–∞–º–∏ –ø–æ—Å—Ç–∞—î –Ω–µ–ø—Ä–æ—Å—Ç–µ –∑–∞–≤–¥–∞–Ω–Ω—è. –ø–æ—á–∞–≤—à–∏ –∑ –ª—ñ–Ω—ñ—ó –∑–∞—Ö–∏—Å—Ç—É, –≤–∞–π–º–∞–∫ –ø–æ—è—Å–Ω–∏–≤ –∫–æ–∂–Ω–æ–º—É –≥—Ä–∞–≤—Ü—é –π–æ–≥–æ –ø—Ä–æ—Ä–∞—Ö—É–Ω–∫–∏ —Ç–∞ –ø–æ—Ö–≤–∞–ª–∏–≤ –∫–æ–º–∞–Ω–¥—É –∑–∞ –Ω–µ—á–∏—Å–ª–µ–Ω–Ω—ñ –≤–¥–∞–ª—ñ –º–æ–º–µ–Ω—Ç–∏.</code> | <code>‚Äî –≤—ñ–Ω –º–∞—î —Ä–∞—Ü—ñ—é, ‚Äî –≤—Ç—Ä—É—Ç–∏–≤—Å—è –Ω—ñ–ª –ø–µ—Ä—à –Ω—ñ–∂ –∫–µ–≤—ñ–Ω –≤—Å—Ç–∏–≥ –≤—ñ–¥–ø–æ–≤—ñ—Å—Ç–∏.</code>                                                                                      | <code>0.0181818181818181</code> |
  | <code>‚Äî –Ω–∞–≤—Ä—è–¥ —á–∏ –∑ –º–µ–Ω–µ –≤–∏–π–¥–µ —Ü—è –ª–∞–Ω–∫–∞, ‚Äî –∑—ñ–∑–Ω–∞–≤—Å—è –Ω—ñ–ª.</code>                                                                                                                                                       | <code>–Ω–∞ —â–∞—Å—Ç—è, —Å—Ç—É–∫—ñ—Ç —É –¥–≤–µ—Ä—ñ –ø–æ–∑–±–∞–≤–∏–≤ –π–æ–≥–æ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—Å—Ç—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—Ç–∏. –Ω—ñ–ª –ø–æ—á–∞–≤ –ø—ñ–¥—ñ–π–º–∞—Ç–∏—Å—è, –∞–ª–µ —á–µ—Ä–µ–∑ –Ω–æ—É—Ç–±—É–∫ –Ω–∞ –∫–æ–ª—ñ–Ω–∞—Ö –∑–∞–±–∞—Ä–∏–≤—Å—è, —ñ –º–µ—Ç—Ç –π–æ–≥–æ –≤–∏–ø–µ—Ä–µ–¥–∏–≤.</code> | <code>0.0833333333333333</code> |
  | <code>–≤—á–æ—Ä–∞ –µ–±–±—ñ –ø–æ–∫–ª–∞–ª–∞ –ø–ª—è—à–µ—á–∫—É –∑ —Ç–≤–æ—ó–º–∏ —Ç–∞–±–ª–µ—Ç–∫–∞–º–∏ –≤ –∞–ø—Ç–µ—á–∫—É –ø–µ—Ä—à–æ—ó –¥–æ–ø–æ–º–æ–≥–∏. —â–æ–π–Ω–æ –∑–∞–ª–∏—à–∞—î—à –ø–æ–ª–µ, –≤–æ–Ω–∏ —Ç–≤–æ—ó.</code>                                                                                               | <code>–ù–∞–ø–µ—Ä–µ–¥–æ–¥–Ω—ñ –µ–±–±—ñ –ø–æ–∫–ª–∞–ª–∞ –ø–ª—è—à–µ—á–∫—É –∑ —Ç–≤–æ—ó–º–∏ —Ç–∞–±–ª–µ—Ç–∫–∞–º–∏ –≤ –∞–ø—Ç–µ—á–∫—É –ø–µ—Ä—à–æ—ó –¥–æ–ø–æ–º–æ–≥–∏ . –¢—ñ–ª—å–∫–∏-–Ω–æ –∑–∞–ª–∏—à–∞—î—à –ø–æ–ª–µ , –≤–æ–Ω–∏ —Ç–≤–æ—ó .</code>                                | <code>0.2727272727272727</code> |
* Loss: [<code>CosineSimilarityLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#cosinesimilarityloss) with these parameters:
  ```json
  {
      "loss_fct": "torch.nn.modules.loss.MSELoss"
  }
  ```

### Training Hyperparameters
#### Non-Default Hyperparameters

- `per_device_train_batch_size`: 16
- `per_device_eval_batch_size`: 16
- `multi_dataset_batch_sampler`: round_robin

#### All Hyperparameters
<details><summary>Click to expand</summary>

- `overwrite_output_dir`: False
- `do_predict`: False
- `eval_strategy`: no
- `prediction_loss_only`: True
- `per_device_train_batch_size`: 16
- `per_device_eval_batch_size`: 16
- `per_gpu_train_batch_size`: None
- `per_gpu_eval_batch_size`: None
- `gradient_accumulation_steps`: 1
- `eval_accumulation_steps`: None
- `torch_empty_cache_steps`: None
- `learning_rate`: 5e-05
- `weight_decay`: 0.0
- `adam_beta1`: 0.9
- `adam_beta2`: 0.999
- `adam_epsilon`: 1e-08
- `max_grad_norm`: 1
- `num_train_epochs`: 3
- `max_steps`: -1
- `lr_scheduler_type`: linear
- `lr_scheduler_kwargs`: {}
- `warmup_ratio`: 0.0
- `warmup_steps`: 0
- `log_level`: passive
- `log_level_replica`: warning
- `log_on_each_node`: True
- `logging_nan_inf_filter`: True
- `save_safetensors`: True
- `save_on_each_node`: False
- `save_only_model`: False
- `restore_callback_states_from_checkpoint`: False
- `no_cuda`: False
- `use_cpu`: False
- `use_mps_device`: False
- `seed`: 42
- `data_seed`: None
- `jit_mode_eval`: False
- `use_ipex`: False
- `bf16`: False
- `fp16`: False
- `fp16_opt_level`: O1
- `half_precision_backend`: auto
- `bf16_full_eval`: False
- `fp16_full_eval`: False
- `tf32`: None
- `local_rank`: 0
- `ddp_backend`: None
- `tpu_num_cores`: None
- `tpu_metrics_debug`: False
- `debug`: []
- `dataloader_drop_last`: False
- `dataloader_num_workers`: 0
- `dataloader_prefetch_factor`: None
- `past_index`: -1
- `disable_tqdm`: False
- `remove_unused_columns`: True
- `label_names`: None
- `load_best_model_at_end`: False
- `ignore_data_skip`: False
- `fsdp`: []
- `fsdp_min_num_params`: 0
- `fsdp_config`: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
- `tp_size`: 0
- `fsdp_transformer_layer_cls_to_wrap`: None
- `accelerator_config`: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
- `deepspeed`: None
- `label_smoothing_factor`: 0.0
- `optim`: adamw_torch
- `optim_args`: None
- `adafactor`: False
- `group_by_length`: False
- `length_column_name`: length
- `ddp_find_unused_parameters`: None
- `ddp_bucket_cap_mb`: None
- `ddp_broadcast_buffers`: False
- `dataloader_pin_memory`: True
- `dataloader_persistent_workers`: False
- `skip_memory_metrics`: True
- `use_legacy_prediction_loop`: False
- `push_to_hub`: False
- `resume_from_checkpoint`: None
- `hub_model_id`: None
- `hub_strategy`: every_save
- `hub_private_repo`: None
- `hub_always_push`: False
- `gradient_checkpointing`: False
- `gradient_checkpointing_kwargs`: None
- `include_inputs_for_metrics`: False
- `include_for_metrics`: []
- `eval_do_concat_batches`: True
- `fp16_backend`: auto
- `push_to_hub_model_id`: None
- `push_to_hub_organization`: None
- `mp_parameters`: 
- `auto_find_batch_size`: False
- `full_determinism`: False
- `torchdynamo`: None
- `ray_scope`: last
- `ddp_timeout`: 1800
- `torch_compile`: False
- `torch_compile_backend`: None
- `torch_compile_mode`: None
- `include_tokens_per_second`: False
- `include_num_input_tokens_seen`: False
- `neftune_noise_alpha`: None
- `optim_target_modules`: None
- `batch_eval_metrics`: False
- `eval_on_start`: False
- `use_liger_kernel`: False
- `eval_use_gather_object`: False
- `average_tokens_across_devices`: False
- `prompts`: None
- `batch_sampler`: batch_sampler
- `multi_dataset_batch_sampler`: round_robin

</details>

### Framework Versions
- Python: 3.12.5
- Sentence Transformers: 4.1.0
- Transformers: 4.51.3
- PyTorch: 2.6.0+cpu
- Accelerate: 1.6.0
- Datasets: 3.5.0
- Tokenizers: 0.21.1

## Citation

### BibTeX

#### Sentence Transformers
```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->